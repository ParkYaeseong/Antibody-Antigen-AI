{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"172oDGaimaaD96z6Q5NICZZrb95McP6vk","authorship_tag":"ABX9TyOC8d5y8XhYdiWQIdFW8SDL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#!pip install torch pandas numpy scikit-learn pyarrow \"numpy<2\""],"metadata":{"collapsed":true,"id":"DkKxaYAKx1ZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!pip install stable-baselines3[extra] gymnasium"],"metadata":{"collapsed":true,"id":"5p61XxUvx8MX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6j0RrDmkfb8j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745984704356,"user_tz":-540,"elapsed":9530,"user":{"displayName":"구글개정","userId":"15305168374719957285"}},"outputId":"d76a0c16-1faa-4dea-e120-2f519d405e90"},"outputs":[{"output_type":"stream","name":"stdout","text":["기본 디렉토리: /content/drive/MyDrive/Antibody_AI_Data_Stage1_AbNumber\n","분할 데이터 로드 경로: /content/drive/MyDrive/Antibody_AI_Data_Stage1_AbNumber/3_split_data\n","모델 저장 경로: /content/drive/MyDrive/Antibody_AI_Data_Stage1_AbNumber/4_trained_models\n","\n","분할된 데이터 로딩 중...\n","데이터 로딩 완료.\n","Train set shape (VH): (6862, 1024)\n","VH, VL 임베딩 결합 완료. Train shape: (6862, 2048)\n","Using device: cpu\n"]}],"source":["# === 필요한 라이브러리 설치 ===\n","# 아나콘다 프롬프트에서 미리 설치하거나, 노트북 셀에서 실행\n","# pip install torch pandas numpy scikit-learn pyarrow \"numpy<2\"\n","# pip install stable-baselines3[extra] gymnasium\n","\n","import os\n","import time\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import gymnasium as gym\n","from gymnasium import spaces\n","\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n","\n","# --- 경로 설정 (로컬 환경 기준) ---\n","# Google Drive 경로가 G: 드라이브에 마운트되었다고 가정\n","try:\n","    # '내 드라이브'에 한글이 포함되어 raw string 또는 슬래시 사용 권장\n","    base_dir = \"/content/drive/MyDrive/Antibody_AI_Data_Stage1_AbNumber\"\n","    # 또는 base_dir = 'G:/내 드라이브/Antibody_AI_Data_Stage1_AbNumber'\n","\n","    if not os.path.exists(base_dir):\n","        raise FileNotFoundError(f\"기본 디렉토리를 찾을 수 없습니다: {base_dir}\")\n","\n","    feature_dir = os.path.join(base_dir, '2_feature_engineered')\n","    split_dir = os.path.join(base_dir, '3_split_data')\n","    model_save_dir = os.path.join(base_dir, '4_trained_models')\n","    os.makedirs(model_save_dir, exist_ok=True)\n","\n","    print(f\"기본 디렉토리: {base_dir}\")\n","    print(f\"분할 데이터 로드 경로: {split_dir}\")\n","    print(f\"모델 저장 경로: {model_save_dir}\")\n","\n","except FileNotFoundError as e:\n","    print(e)\n","    # 필요한 경우 스크립트 중단 또는 기본 경로 설정\n","    exit()\n","except Exception as e:\n","    print(f\"경로 설정 중 오류 발생: {e}\")\n","    exit()\n","\n","\n","# --- 데이터 로딩 ---\n","print(\"\\n분할된 데이터 로딩 중...\")\n","try:\n","    # 사용할 모델 임베딩 파일명 확인 (예: IgBert 사용 시)\n","    # 이전 단계에서 저장한 정확한 파일명 사용 필요\n","    model_name_safe = \"Exscientia_IgBert\" # 예시, 실제 사용한 모델명으로 변경\n","    vh_train_path = os.path.join(split_dir, f'X_train_vh.npy') # 경로 수정\n","    vl_train_path = os.path.join(split_dir, f'X_train_vl.npy') # 경로 수정\n","\n","    # 실제 임베딩 파일명 생성 로직 (이전 단계 참고)\n","    # vh_embeddings_path = os.path.join(feature_dir, f'vh_embeddings_{model_name_safe}.npy')\n","    # vl_embeddings_path = os.path.join(feature_dir, f'vl_embeddings_{model_name_safe}.npy')\n","    # 위 경로는 전체 임베딩 파일이고, split_dir 아래에 분할된 npy 파일이 있어야 함.\n","\n","    X_train_vh = np.load(vh_train_path)\n","    X_train_vl = np.load(vl_train_path)\n","    # X_val_vh = np.load(os.path.join(split_dir, 'X_val_vh.npy'))\n","    # X_val_vl = np.load(os.path.join(split_dir, 'X_val_vl.npy'))\n","    # X_test_vh = np.load(os.path.join(split_dir, 'X_test_vh.npy'))\n","    # X_test_vl = np.load(os.path.join(split_dir, 'X_test_vl.npy'))\n","\n","    metadata_train = pd.read_parquet(os.path.join(split_dir, 'metadata_train.parquet'))\n","    # metadata_val = pd.read_parquet(os.path.join(split_dir, 'metadata_val.parquet'))\n","    # metadata_test = pd.read_parquet(os.path.join(split_dir, 'metadata_test.parquet'))\n","\n","    print(\"데이터 로딩 완료.\")\n","    print(\"Train set shape (VH):\", X_train_vh.shape)\n","\n","except FileNotFoundError as e:\n","    print(f\"오류: 분할된 데이터 파일을 찾을 수 없습니다. 경로를 확인하세요: {e}\")\n","    raise\n","except Exception as e:\n","    print(f\"데이터 로딩 중 오류 발생: {e}\")\n","    raise\n","\n","# --- 데이터 준비 ---\n","# VH, VL 임베딩 결합 (예: 단순 연결)\n","X_train = np.concatenate((X_train_vh, X_train_vl), axis=1)\n","# X_val = np.concatenate((X_val_vh, X_val_vl), axis=1)\n","# X_test = np.concatenate((X_test_vh, X_test_vl), axis=1)\n","print(\"VH, VL 임베딩 결합 완료. Train shape:\", X_train.shape)\n","\n","# GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","source":["# --- 속성 예측 모델 로드 또는 정의 ---\n","\n","# 실제 구현 시에는 이 부분에 미리 학습된 모델을 로드하거나,\n","# 라벨 데이터가 있다면 여기서 모델을 정의하고 학습시켜야 합니다.\n","\n","# 예시 1: 미리 학습된 모델 로드 (파일이 존재한다고 가정)\n","# predictor_model_path = os.path.join(model_save_dir, 'property_predictor_model.pth')\n","# if os.path.exists(predictor_model_path):\n","#     input_dim = X_train.shape[1]\n","#     predictor_model = PropertyPredictor(input_dim=input_dim).to(device) # Phase 2에서 정의한 모델 클래스 필요\n","#     try:\n","#         predictor_model.load_state_dict(torch.load(predictor_model_path, map_location=device))\n","#         predictor_model.eval()\n","#         print(f\"속성 예측 모델 로드 완료: {predictor_model_path}\")\n","#     except Exception as e:\n","#         print(f\"경고: 속성 예측 모델 로드 실패 ({e}). 플레이스홀더 보상을 사용합니다.\")\n","#         predictor_model = None\n","# else:\n","#     print(\"경고: 학습된 속성 예측 모델 파일을 찾을 수 없습니다. 플레이스홀더 보상을 사용합니다.\")\n","#     predictor_model = None\n","\n","# 예시 2: 플레이스홀더 함수 정의 (실제 모델 없을 경우)\n","# 이 함수는 실제 최적화 목표에 맞게 반드시 수정되어야 합니다!\n","def placeholder_property_predictor(embedding_tensor):\n","    \"\"\"\n","    임베딩을 받아 임의의 속성 점수(보상 관련)를 반환하는 플레이스홀더 함수.\n","    실제 예측 모델로 교체하거나, 다른 보상 계산 로직(예: pLM likelihood)으로 대체 필요.\n","    \"\"\"\n","    # 예시: 특정 타겟 임베딩과의 거리 (거리가 가까울수록 높은 점수)\n","    # target_embedding = torch.randn_like(embedding_tensor) # 실제로는 목표 임베딩 사용\n","    # score = -torch.norm(embedding_tensor - target_embedding, dim=1) # 거리의 음수값\n","    # return score.item()\n","\n","    # 예시 2: 임베딩 벡터의 특정 부분 합계 (단순 예시)\n","    # score = torch.sum(embedding_tensor[:, :10], dim=1)\n","    # return score.item()\n","\n","    # 가장 간단한 예시: 랜덤 점수 반환\n","    return np.random.rand() * 10 # 0 ~ 10 사이 랜덤 점수\n","\n","\n","predictor_model = placeholder_property_predictor # 플레이스홀더 함수 사용\n","print(\"알림: 실제 속성 예측 모델 대신 플레이스홀더 보상 함수를 사용합니다.\")\n","print(\"      => 의미있는 최적화를 위해서는 이 부분을 반드시 수정해야 합니다.\")"],"metadata":{"id":"nX1WLMnJfiPd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745984709908,"user_tz":-540,"elapsed":20,"user":{"displayName":"구글개정","userId":"15305168374719957285"}},"outputId":"6d63439b-8182-4254-9662-c02c356942ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["알림: 실제 속성 예측 모델 대신 플레이스홀더 보상 함수를 사용합니다.\n","      => 의미있는 최적화를 위해서는 이 부분을 반드시 수정해야 합니다.\n"]}]},{"cell_type":"code","source":["# --- 강화학습 환경 정의 (Gymnasium Env 상속) ---\n","class AntibodyOptimizeEnv(gym.Env):\n","    \"\"\"\n","    항체 임베딩 최적화를 위한 커스텀 Gymnasium 환경.\n","    State: 현재 항체 임베딩 (VH+VL 결합).\n","    Action: 임베딩 벡터에 작은 변화를 주는 벡터 (연속 공간).\n","    Reward: 속성 예측 모델의 출력 또는 다른 지표 (높을수록 좋음).\n","    \"\"\"\n","    metadata = {'render_modes': ['human']} # 필요시 렌더링 모드 정의\n","\n","    def __init__(self, initial_embeddings_pool, predictor_fn, max_steps=100, action_scale=0.01):\n","        \"\"\"\n","        Args:\n","            initial_embeddings_pool (np.ndarray): 초기 상태로 사용할 임베딩 풀 (예: X_train).\n","            predictor_fn (callable): 임베딩 텐서를 입력받아 속성 점수를 반환하는 함수.\n","            max_steps (int): 한 에피소드의 최대 스텝 수.\n","            action_scale (float): 액션(변화량)의 크기를 조절하는 스케일 팩터.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.embeddings_pool = initial_embeddings_pool\n","        self.predictor_fn = predictor_fn\n","        self.max_steps = max_steps\n","        self.action_scale = action_scale\n","        self.embedding_dim = initial_embeddings_pool.shape[1]\n","\n","        # Observation Space: 임베딩 벡터 (연속 공간)\n","        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n","                                            shape=(self.embedding_dim,), dtype=np.float32)\n","\n","        # Action Space: 임베딩 벡터에 더할 변화량 (연속 공간, -1 ~ 1 범위)\n","        # 실제 적용 시에는 action * action_scale 만큼 변화시킴\n","        self.action_space = spaces.Box(low=-1, high=1,\n","                                       shape=(self.embedding_dim,), dtype=np.float32)\n","\n","        self.current_step = 0\n","        self.current_embedding = None\n","\n","    def _get_initial_embedding(self):\n","        # 풀에서 무작위로 초기 임베딩 선택\n","        idx = np.random.randint(len(self.embeddings_pool))\n","        return self.embeddings_pool[idx].copy()\n","\n","    def reset(self, seed=None, options=None):\n","        # 환경 초기화\n","        super().reset(seed=seed) # Gymnasium 표준 시드 설정\n","        self.current_embedding = self._get_initial_embedding()\n","        self.current_step = 0\n","        observation = self.current_embedding\n","        info = {} # 추가 정보 (필요시)\n","        # print(\"Environment reset.\") # 디버깅용\n","        return observation, info\n","\n","    def step(self, action):\n","        # 1. Action 적용 (임베딩 수정)\n","        # action은 -1 ~ 1 범위, action_scale로 실제 변화량 조절\n","        perturbation = action * self.action_scale\n","        self.current_embedding += perturbation\n","        # (선택사항) 임베딩 값 범위 제한 등 후처리 가능\n","\n","        self.current_step += 1\n","\n","        # 2. Reward 계산\n","        # 현재 임베딩을 Tensor로 변환하여 예측 함수에 전달\n","        embedding_tensor = torch.tensor(self.current_embedding, dtype=torch.float32).unsqueeze(0).to(device)\n","        # predictor_fn 이 Tensor를 직접 처리 못하면 numpy로 변환 후 전달 필요\n","        # 예: reward = self.predictor_fn(self.current_embedding)\n","        reward = self.predictor_fn(embedding_tensor) # predictor_fn의 반환 타입 확인 필요 (스칼라 값이어야 함)\n","\n","        # 3. 종료 조건 확인 (Done)\n","        terminated = self.current_step >= self.max_steps\n","        truncated = False # 시간 제한 외 다른 이유로 잘리는 경우 (여기서는 사용 안함)\n","\n","        # 4. 추가 정보 (Info) - 디버깅 등에 활용\n","        info = {}\n","\n","        # 상태, 보상, 종료 여부 반환\n","        observation = self.current_embedding\n","        # print(f\"Step: {self.current_step}, Reward: {reward:.4f}\") # 디버깅용\n","        return observation, reward, terminated, truncated, info\n","\n","    def render(self):\n","        # 시각화 로직 (필요시 구현)\n","        pass\n","\n","    def close(self):\n","        # 환경 정리 작업 (필요시 구현)\n","        pass\n","\n","\n","# --- 병렬 환경 생성 및 에이전트 학습 ---\n","if __name__ == '__main__': # 멀티프로세싱 사용 시 필요\n","\n","    # 병렬 환경 수 설정 (CPU 코어 수 고려)\n","    num_cpu = os.cpu_count()\n","    n_envs = max(1, num_cpu - 2) if num_cpu else 1 # 사용 가능한 CPU 코어보다 적게 설정 권장\n","    print(f\"\\n사용할 병렬 환경 수: {n_envs}\")\n","\n","    # 환경 생성 함수 정의\n","    # 초기 임베딩 풀(X_train), 예측 함수(predictor_model) 등을 인자로 전달\n","    env_kwargs = {\n","        'initial_embeddings_pool': X_train,\n","        'predictor_fn': predictor_model, # 실제 예측 모델 또는 개선된 보상 함수로 교체!\n","        'max_steps': 200, # 에피소드 최대 길이 조절 필요\n","        'action_scale': 0.01 # 임베딩 변화 크기 조절 필요\n","    }\n","\n","    try:\n","        # 병렬 환경 생성 (SubprocVecEnv 사용)\n","        vec_env = make_vec_env(\n","            lambda: AntibodyOptimizeEnv(**env_kwargs),\n","            n_envs=n_envs,\n","            vec_env_cls=SubprocVecEnv # 멀티프로세스 기반 병렬 환경\n","            # vec_env_cls=DummyVecEnv # 디버깅 시 사용 (단일 프로세스)\n","        )\n","        print(\"병렬 환경 생성 완료.\")\n","\n","        # PPO 에이전트 정의\n","        # MlpPolicy: 표준 MLP 기반 정책/가치 네트워크\n","        # verbose=1: 학습 진행 상황 출력\n","        # device='auto': 사용 가능한 경우 GPU 자동 사용\n","        # n_steps, batch_size, learning_rate 등 주요 하이퍼파라미터 튜닝 필요\n","        rl_model = PPO(\"MlpPolicy\", vec_env, verbose=1, device='auto',\n","                       n_steps=1024, batch_size=64, n_epochs=10, learning_rate=3e-4)\n","        print(\"PPO 에이전트 정의 완료.\")\n","\n","        # 에이전트 학습\n","        total_timesteps = 200000 # 총 학습 타임스텝 (충분히 길게 설정 필요)\n","        print(f\"\\n--- 강화학습 에이전트 학습 시작 (총 {total_timesteps} 타임스텝) ---\")\n","        start_time_rl = time.time()\n","        rl_model.learn(total_timesteps=total_timesteps, progress_bar=True) # 진행바 표시\n","        end_time_rl = time.time()\n","        print(f\"--- 강화학습 에이전트 학습 완료 (소요 시간: {end_time_rl - start_time_rl:.2f} 초) ---\")\n","\n","        # 학습된 모델 저장\n","        model_save_path = os.path.join(model_save_dir, \"antibody_ppo_agent\")\n","        rl_model.save(model_save_path)\n","        print(f\"학습된 RL 에이전트 저장 완료: {model_save_path}\")\n","\n","        # 병렬 환경 종료\n","        vec_env.close()\n","\n","    except Exception as e:\n","        print(f\"\\n강화학습 실행 중 오류 발생: {e}\")\n","        # 오류 발생 시 생성된 환경 닫기 시도\n","        if 'vec_env' in locals() and vec_env is not None:\n","            try:\n","                vec_env.close()\n","                print(\"오류 발생 후 병렬 환경 종료 시도.\")\n","            except Exception as close_e:\n","                print(f\"병렬 환경 종료 중 추가 오류: {close_e}\")\n","        raise # 오류 다시 발생시켜 확인"],"metadata":{"id":"_3_mw1Hafkju","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"33d7cf3d-aeb0-4e4a-cb64-47778391ef19"},"execution_count":null,"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  40%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">79,846/200,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:05:27</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:08:03</span> , <span style=\"color: #800000; text-decoration-color: #800000\">249 it/s</span> ]\n","</pre>\n"],"text/plain":["\u001b[35m  40%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79,846/200,000 \u001b[0m [ \u001b[33m0:05:27\u001b[0m < \u001b[36m0:08:03\u001b[0m , \u001b[31m249 it/s\u001b[0m ]\n"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","source":["# --- 학습된 RL 에이전트 활용 (예시) ---\n","\n","# 1. 모델 로드 (필요시)\n","# rl_model = PPO.load(model_save_path, device=device)\n","\n","# 2. 특정 시작 임베딩으로 최적화 실행\n","# initial_embedding = X_test[0].copy() # 예: 테스트셋의 첫 번째 임베딩\n","# obs, _ = env.reset() # 단일 환경 필요 (또는 VecEnv의 reset 사용)\n","# obs[:] = initial_embedding # VecEnv 사용 시 상태 설정 방식 다름\n","\n","# optimized_embeddings = []\n","# current_obs = initial_embedding.copy() # 관찰 상태 초기화\n","\n","# # 단일 환경에서 에피소드 실행 (병렬 환경 아님)\n","# single_env = AntibodyOptimizeEnv(**env_kwargs) # 평가용 단일 환경\n","# obs, _ = single_env.reset()\n","# obs = initial_embedding # 시작 상태 설정\n","\n","# for _ in range(env_kwargs['max_steps']):\n","#     action, _states = rl_model.predict(obs, deterministic=True) # 결정론적 액션 선택\n","#     obs, reward, terminated, truncated, info = single_env.step(action)\n","#     print(f\"  Eval Step Reward: {reward:.4f}\")\n","#     if terminated or truncated:\n","#         break\n","# optimized_embeddings.append(obs.copy())\n","\n","# print(\"\\n최적화된 임베딩 (예시):\", optimized_embeddings[-1][:10]) # 마지막 임베딩 일부 출력\n","\n","# 3. 결과 분석\n","# - 얻어진 optimized_embeddings 들의 속성 예측값 확인\n","# - 초기 임베딩 대비 얼마나 개선되었는지 평가\n","# - (고급) 임베딩을 다시 서열로 변환하는 방법 필요 (예: Decoder 모델, 가장 가까운 실제 서열 찾기 등)\n","\n","print(\"\\n--- 평가 및 활용 단계 (개념적 설명) ---\")\n","print(\"학습된 에이전트를 사용하여 새로운 임베딩을 생성/최적화하고,\")\n","print(\"그 결과를 속성 예측 모델 등으로 평가해야 합니다.\")\n","print(\"임베딩을 서열로 변환하는 디코딩 단계가 필요할 수 있습니다.\")"],"metadata":{"id":"--TdspZ8ic7-"},"execution_count":null,"outputs":[]}]}